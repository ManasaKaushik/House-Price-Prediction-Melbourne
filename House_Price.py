# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oMhiIt2c1jNAUL0gtYxgeGArexjqvJQZ
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
mel_pless = pd.read_csv('MELBOURNE_HOUSE_PRICES_LESS.csv')
mel_full = pd.read_csv('Melbourne_housing_FULL.csv')
mel_full.columns

mel_full = mel_full.dropna(axis=0)

y = mel_full.Price

house_features = ['Rooms', 'Bedroom2', 'Bathroom', 'YearBuilt', 'Landsize', 'Lattitude', 'Longtitude']
X = mel_full[house_features]
X.describe()
X.head()

from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor(random_state=7)
model.fit(X,y)

print('The price predictions are:')
v = model.predict(X)
print(v)

from sklearn.metrics import mean_absolute_error
mean_absolute_error(y,v)

#Since the model is trained on the dataset entirely and is then tested on the
#dataset again, the validation of the model does not hold good because it already has
#access to the values it is going to predict. So we will use train_test_split here to
#have a different part of the dataset to test the model on.
from sklearn.model_selection import train_test_split
train_X,val_X,train_y,val_y = train_test_split(X,y,random_state=7)
model_tts = DecisionTreeRegressor(random_state=7)
model_tts.fit(train_X,train_y)
pred = model_tts.predict(val_X)
mean_absolute_error(val_y, pred)

#from such a high value difference of the predicted price vs the actual price, we understand
#that the model is overshooting the values. This is a topic of underfitting/overfitting.
#This can be controlled by observing the number of leaf nodes in the decision tree. Too many
#leaf nodes will cause overfitting while too little will cause underfitting.
def find_best(max_leaf_nodes, train_X, val_X, train_y, val_y):
  model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=7)
  model.fit(train_X,train_y)
  prediction = model.predict(val_X)
  mae = mean_absolute_error(val_y, prediction)
  return mae

#for different values of the leaf nodes, we can run a for loop and check which one fits the best
for max_leaf_nodes in [50,100,200,400,800,1500,3000,5000]:
  eval = find_best(max_leaf_nodes, train_X, val_X, train_y, val_y)
  print("Max leaf nodes: %d \t\t Mean Absolute Error: %d" %(max_leaf_nodes, eval))

#To dig down further, we can narrow down the loop from 300 to 800
candidate_max_leaf_nodes = [300,350,400,450,500,550,600,650,700,750,800]
scores = {leaf_size: find_best(leaf_size, train_X, val_X, train_y, val_y) for leaf_size in candidate_max_leaf_nodes}
best_tree_size = min(scores, key=scores.get)
print(best_tree_size)
#We can confirm that the ideal number of leaf nodes are 400. Since we have the stats down, we can train the model
#on the entire dataset, instead of a part of it. 
final_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size,random_state=7)
final_model.fit(X,y)
r = final_model.predict(X)
mean_absolute_error(r,y)
#thus, we have brought down the mean aboslute error by atleast 50%, indicating that
#the validation accuracy has increased by 50%

